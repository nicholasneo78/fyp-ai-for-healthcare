{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6da78a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\nicholasneo78\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import preprocessor as p # tweet-preprocessor\n",
    "import cleantext\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.stem.WordNetLemmatizer().lemmatize('word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5c55fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENGLISH_STOP_WORDS_LIST = list(ENGLISH_STOP_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f18fb506",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['while', 'yet', 'those', 'empty', 'becomes', 'whereupon', 'every', 'why', 'although', 'our', 'how', 'otherwise', 'us', 'their', 'hundred', 'beyond', 'same', 'eight', 'has', 'bottom', 'within', 'sometime', 'though', 'about', 'third', 'across', 'anywhere', 'in', 're', 'front', 'each', 'thin', 'etc', 'describe', 'serious', 'before', 'thick', 'been', 'very', 'these', 'none', 'toward', 'during', 'bill', 'ever', 'next', 'except', 'another', 'them', 'most', 'no', 'where', 'almost', 'of', 'onto', 'two', 'also', 'forty', 'more', 'elsewhere', 'fifty', 'because', 'both', 'fill', 'hereby', 'thus', 'became', 'now', 'per', 'whence', 'the', 'but', 'un', 'thereafter', 'therefore', 'would', 'you', 'becoming', 'sometimes', 'thereby', 'nine', 'eg', 'meanwhile', 'whereafter', 'yourself', 'much', 'three', 'i', 'who', 'we', 'latter', 'or', 'system', 'must', 'against', 'namely', 'whether', 'any', 'here', 'below', 'is', 'what', 'enough', 'him', 'than', 'least', 'as', 'cry', 'last', 'wherein', 'hereafter', 'me', 'else', 'they', 'ie', 'too', 'nothing', 'often', 'therein', 'other', 'not', 'either', 'herein', 'detail', 'around', 'whenever', 'it', 'others', 'through', 'with', 'anyhow', 'one', 'mill', 'less', 'since', 'ours', 'already', 'de', 'done', 'up', 'yours', 'inc', 'alone', 'eleven', 'towards', 'this', 'amoungst', 'can', 'few', 'mine', 'until', 'off', 'perhaps', 'well', 'call', 'own', 'at', 'still', 'after', 'afterwards', 'back', 'behind', 'whither', 'itself', 'four', 'together', 'upon', 'your', 'co', 'amount', 'interest', 'and', 'beside', 'over', 'cannot', 'hence', 'his', 'yourselves', 'have', 'only', 'indeed', 'my', 'get', 'under', 'former', 'formerly', 'latterly', 'however', 'neither', 'himself', 'even', 'find', 'top', 'take', 'whatever', 'being', 'many', 'ourselves', 'be', 'side', 'herself', 'above', 'someone', 'could', 'fifteen', 'name', 'somehow', 'made', 'part', 'always', 'she', 'see', 'all', 'among', 'wherever', 'sincere', 'seems', 'by', 'become', 'nevertheless', 'cant', 'its', 'had', 'first', 'hereupon', 'further', 'somewhere', 'rather', 'that', 'via', 'ten', 'found', 'should', 'anyone', 'nowhere', 'thence', 'everything', 'were', 'something', 'when', 'moreover', 'mostly', 'are', 'noone', 'put', 'he', 'ltd', 'seemed', 'whom', 'sixty', 'am', 'con', 'anything', 'some', 'six', 'was', 'will', 'which', 'down', 'might', 'without', 'hers', 'thru', 'whereas', 'give', 'between', 'amongst', 'nor', 'if', 'anyway', 'a', 'nobody', 'never', 'please', 'everyone', 'to', 'several', 'move', 'into', 'themselves', 'whose', 'due', 'couldnt', 'everywhere', 'then', 'there', 'along', 'go', 'from', 'beforehand', 'seem', 'keep', 'may', 'twenty', 'on', 'seeming', 'an', 'whole', 'her', 'for', 'such', 'myself', 'whoever', 'once', 'out', 'hasnt', 'again', 'whereby', 'thereupon', 'besides', 'show', 'so', 'fire', 'twelve', 'full', 'do', 'five', 'throughout']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "317"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(ENGLISH_STOP_WORDS_LIST)\n",
    "# remove 'not' as it is quite an important contrast word\n",
    "ENGLISH_STOP_WORDS_LIST.remove('not') # only run once\n",
    "len(ENGLISH_STOP_WORDS_LIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e71f2139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the helper functions to clean the data depending on which tasks\n",
    "\n",
    "# https://en.wikipedia.org/wiki/Wikipedia%3aList_of_English_contractions\n",
    "# remove contractions\n",
    "def contraction_removal(phrase):\n",
    "    # replace bad characters\n",
    "    phrase = phrase.replace(u'’', u\"'\")\n",
    "    phrase = phrase.replace(u'‘', u\"'\")\n",
    "    # more specific change\n",
    "    phrase = re.sub(r\"won\\'t\", \" will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \" cannot\", phrase)\n",
    "    phrase = re.sub(r\"shan\\'t\", \" shall not\", phrase)\n",
    "    phrase = re.sub(r\"I ain\\t\", \" I am not\", phrase)\n",
    "    phrase = re.sub(r\"i ain\\t\", \" I am not\", phrase)\n",
    "    phrase = re.sub(r\"She ain\\t\", \" she is not\", phrase)\n",
    "    phrase = re.sub(r\"He ain\\t\", \" he is not\", phrase)\n",
    "    phrase = re.sub(r\"he ain\\t\", \" he am not\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase\n",
    "\n",
    "# remove url, mention, reserved words, emoji, smiley and number\n",
    "# using tweet preprocessor library here\n",
    "def tweet_preprocessor(text, config):\n",
    "    if config == 'deep_clean' or config == 'ecpe':\n",
    "        p.set_options(p.OPT.URL, p.OPT.MENTION, p.OPT.EMOJI, p.OPT.RESERVED, p.OPT.SMILEY)\n",
    "    elif config == 'vader':\n",
    "        p.set_options(p.OPT.URL, p.OPT.MENTION, p.OPT.RESERVED, p.OPT.SMILEY)\n",
    "    text = p.clean(text)\n",
    "    # remove the url starting with www\n",
    "    text = re.sub(r\"\\bwww.\\w+\", \"\", text)\n",
    "    # just remove hashtag (not the whole hashtag and words)\n",
    "    text = re.sub(r\"#\", \" \", text)\n",
    "    return text\n",
    "\n",
    "# remove extra spaces and stopwords, do lowercase, and remove punctuations.\n",
    "def clean_text(text, removeLower=True, removeNumbers=True, removePunct=True, removeExtraSpace=True):\n",
    "    text = cleantext.clean(text, \n",
    "                    lowercase=removeLower, \n",
    "                    numbers=removeNumbers, \n",
    "                    punct=removePunct,\n",
    "                    extra_spaces=removeExtraSpace)\n",
    "    return str(text)\n",
    "\n",
    "# keep only alphabets (only for deep clean)\n",
    "def keep_alphabet_only(text):\n",
    "    return re.sub('[^a-zA-Z- ]+', '', text)\n",
    "\n",
    "# keep alphabets, some basic punctuations and numbera\n",
    "def keep_selected(text):\n",
    "    emoji_pat = '[\\U0001F300-\\U0001F64F\\U0001F680-\\U0001F6FF\\u2600-\\u26FF\\u2700-\\u27BF]'\n",
    "    shrink_whitespace_reg = re.compile(r'\\s{2,}')\n",
    "    reg = re.compile(r'({})|[^a-zA-Z0-9,.!?-]'.format(emoji_pat)) # line a\n",
    "    result = reg.sub(lambda x: ' {} '.format(x.group(1)) if x.group(1) else ' ', text)\n",
    "    return shrink_whitespace_reg.sub(' ', result)\n",
    "\n",
    "# eliminate letters who appeared more than twice in the text\n",
    "def eliminate_multi_letters(text):\n",
    "    return re.sub(r'(.)\\1{2,}', r'\\1\\1', text)\n",
    "'''\n",
    "# autocorrect to fix spelling errors\n",
    "spell = Speller()\n",
    "def autocorrect(text):\n",
    "    return spell(text)\n",
    "'''\n",
    "\n",
    "def stopword_removal(text):\n",
    "    words = [word for word in text.split() if word.lower() not in ENGLISH_STOP_WORDS_LIST]\n",
    "    text = \" \".join(words)\n",
    "    return text\n",
    "\n",
    "# perform lemmatization here\n",
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "# lemmatize text\n",
    "def lemmatize_text(text):\n",
    "    text_list = [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)]\n",
    "    listToStr = ' '.join([str(elem) for elem in text_list])\n",
    "    return listToStr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce983022",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26db301c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform lemmatization here\n",
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982d05b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6b9e02b1",
   "metadata": {},
   "source": [
    "## Deep Clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61665beb",
   "metadata": {},
   "source": [
    "To generate the following files:   \n",
    "emotion_classification_cleaned_toy_data_{type}.csv (train and dev)  \n",
    "\n",
    "emotion_classification_cleaned_long_data_{type}.csv (train and dev)   \n",
    "emotion_classification_cleaned_short_data_{type}.csv (train and dev)  \n",
    "emotion_classification_cleaned_combined_data_{type}.csv (train and dev)      \n",
    "  \n",
    "emotion_intensity_wassa_sadness_combined_{type}.csv (train and dev)   \n",
    "emotion_intensity_wassa_anger_combined_{type}.csv (train and dev)   \n",
    "emotion_intensity_wassa_fear_combined_{type}.csv (train and dev)   \n",
    "   \n",
    "Only depressed data here  \n",
    "emotion_intensity_depressed_clean_long_data_test.csv (test set for machine learning portion)  \n",
    "emotion_intensity_depressed_clean_short_data_test.csv (test set for machine learning portion)  \n",
    "  \n",
    "Total: 16 data  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8a6a278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the deep clean data\n",
    "def deep_clean(text):\n",
    "    text = contraction_removal(text)\n",
    "    text = tweet_preprocessor(text, 'deep_clean')\n",
    "    text = stopword_removal(text)\n",
    "    text = clean_text(text, \n",
    "                       removeLower=True, \n",
    "                       removeNumbers=True, \n",
    "                       removePunct=True, \n",
    "                       removeExtraSpace=True)\n",
    "    text = keep_alphabet_only(text)\n",
    "    text = eliminate_multi_letters(text)\n",
    "    text = lemmatize_text(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8804e046",
   "metadata": {},
   "source": [
    "## Vader and text2emotion data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0215fd3",
   "metadata": {},
   "source": [
    "To generate the following files:  \n",
    "emotion_intensity_depressed_clean_long_data_vader_t2e.csv  \n",
    "emotion_intensity_depressed_clean_short_data_vader_t2e.csv\n",
    "\n",
    "Total: 2 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "248b18f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the vader and t2e data\n",
    "def vader_and_t2e_clean(text):\n",
    "    text = contraction_removal(text)\n",
    "    text = tweet_preprocessor(text, 'vader')\n",
    "    text = clean_text(text, \n",
    "                      removeLower=False, \n",
    "                      removeNumbers=True, \n",
    "                      removePunct=False, \n",
    "                      removeExtraSpace=True)\n",
    "    text = keep_selected(text)\n",
    "    text = eliminate_multi_letters(text)\n",
    "    text = lemmatize_text(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7e5714",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5959c36e",
   "metadata": {},
   "source": [
    "## ECPE data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08be4310",
   "metadata": {},
   "source": [
    "To generate the following file:  \n",
    "ecpe_cleaned_long_data.csv\n",
    "\n",
    "Total: 1 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71a044d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ecpe_clean(text):\n",
    "    text = contraction_removal(text)\n",
    "    text = tweet_preprocessor(text, 'ecpe')\n",
    "    text = clean_text(text, \n",
    "                      removeLower=False, \n",
    "                      removeNumbers=False, \n",
    "                      removePunct=False, \n",
    "                      removeExtraSpace=True)\n",
    "    text = eliminate_multi_letters(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69d71e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da24bec0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0642126c",
   "metadata": {},
   "source": [
    "## Load all the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d8ac8621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the raw data in\n",
    "toy_data = pd.read_csv('./data/toy_data.csv')\n",
    "long_text = pd.read_csv('./data/long_text_combined.csv')\n",
    "long_text_only_depressed = pd.read_csv('./data/long_text_only_depressed.csv')\n",
    "short_text = pd.read_csv('./data/short_text.csv')\n",
    "short_text_only_depressed = pd.read_csv('./data/short_text_only_depressed.csv')\n",
    "wassa_anger_train_raw = pd.read_csv('./data/wassa_anger_train_raw.csv')\n",
    "wassa_anger_dev_raw = pd.read_csv('./data/wassa_anger_dev_raw.csv')\n",
    "wassa_fear_train_raw = pd.read_csv('./data/wassa_fear_train_raw.csv')\n",
    "wassa_fear_dev_raw = pd.read_csv('./data/wassa_fear_dev_raw.csv')\n",
    "wassa_sadness_train_raw = pd.read_csv('./data/wassa_sadness_train_raw.csv')\n",
    "wassa_sadness_dev_raw = pd.read_csv('./data/wassa_sadness_dev_raw.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92860250",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c16e133f",
   "metadata": {},
   "source": [
    "## Cleaning the data and export it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f2a9c1",
   "metadata": {},
   "source": [
    "### Toy Dataset for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e225eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplication of entries\n",
    "toy_data=toy_data.drop_duplicates(subset={\"Text\",\"Label\"}, \n",
    "                                              keep='first', inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "054b569b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 10282/10282 [00:05<00:00, 1965.58it/s]\n"
     ]
    }
   ],
   "source": [
    "# cleaning of data\n",
    "text_list = []\n",
    "for i in tqdm(toy_data['Text']):\n",
    "    temp = deep_clean(i)\n",
    "    text_list.append(temp)\n",
    "toy_data['text_cleaned'] = text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b55436e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove sentences with 1 word or less after stopword removal\n",
    "toy_data = toy_data[toy_data['text_cleaned'].str.split().str.len().gt(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "86304c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the temporary backup file\n",
    "toy_data.to_csv('./data/backup_cleaned_data/toy_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1581ce5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    7663\n",
       "1    2261\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for data imbalance\n",
    "#print(toy_data.head(2))\n",
    "toy_data['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "70fc3743",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    2261\n",
       "0    2261\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# settle data imbalance issue\n",
    "# toy data\n",
    "toy_data_0 = toy_data[toy_data['Label'] == 0]\n",
    "toy_data_1 = toy_data[toy_data['Label'] == 1]\n",
    "\n",
    "# downsample the data of non-depression class to fit the number of the depression class\n",
    "toy_data_0_down = toy_data[toy_data['Label'] == 0].sample(len(toy_data_1), replace=False)\n",
    "#print(toy_data_0_down.shape)\n",
    "\n",
    "# concatenate back the data into 1 dataset\n",
    "frames = [toy_data_0_down, toy_data_1]\n",
    "toy_data_balanced = pd.concat(frames)\n",
    "\n",
    "# shuffle dataset for better data distribution\n",
    "toy_data_balanced = toy_data_balanced.sample(frac=1).reset_index(drop=True)\n",
    "#toy_data_balanced.head(3)\n",
    "toy_data_balanced['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "13858918",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "      <th>text_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@AlanMusselman I know right. Yes I will send i...</td>\n",
       "      <td>0</td>\n",
       "      <td>know right yes send eod</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Depression love me so much</td>\n",
       "      <td>1</td>\n",
       "      <td>depression love</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Label  \\\n",
       "0  @AlanMusselman I know right. Yes I will send i...      0   \n",
       "1                         Depression love me so much      1   \n",
       "\n",
       "              text_cleaned  \n",
       "0  know right yes send eod  \n",
       "1          depression love  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toy_data_balanced.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3b22184b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train dev split the data with stratified sampling\n",
    "toy_data_train, toy_data_dev = train_test_split(toy_data_balanced, \n",
    "                                                train_size=0.8, \n",
    "                                                random_state=42, \n",
    "                                                stratify=toy_data_balanced['Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a5a70604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    1809\n",
      "0    1808\n",
      "Name: Label, dtype: int64\n",
      "0    453\n",
      "1    452\n",
      "Name: Label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(toy_data_train['Label'].value_counts())\n",
    "print(toy_data_dev['Label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "924e0359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export both the train and the dev data\n",
    "toy_data_train.to_csv('./data/emotion_classification/emotion_classification_cleaned_toy_data_train.csv', index=False)\n",
    "toy_data_dev.to_csv('./data/emotion_classification/emotion_classification_cleaned_toy_data_dev.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc888ac5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ac2c5892",
   "metadata": {},
   "source": [
    "### Short Dataset for Classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dc7ca61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplication of entries\n",
    "short_text=short_text.drop_duplicates(subset={\"Text\",\"Label\"}, \n",
    "                                              keep='first', inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9ab22587",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 3099/3099 [00:01<00:00, 1936.84it/s]\n"
     ]
    }
   ],
   "source": [
    "# cleaning of data\n",
    "text_list = []\n",
    "for i in tqdm(short_text['Text']):\n",
    "    temp = deep_clean(i)\n",
    "    text_list.append(temp)\n",
    "short_text['text_cleaned'] = text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "84cd63ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2192\n",
       "1     834\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove sentences with 1 word or less after stopword removal\n",
    "short_text = short_text[short_text['text_cleaned'].str.split().str.len().gt(1)]\n",
    "\n",
    "# save the temporary backup file\n",
    "short_text.to_csv('./data/backup_cleaned_data/short_text.csv', index=False)\n",
    "\n",
    "# check for data imbalance\n",
    "short_text['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fa89fe20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    834\n",
       "0    834\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# short text data\n",
    "short_text_0 = short_text[short_text['Label'] == 0]\n",
    "short_text_1 = short_text[short_text['Label'] == 1]\n",
    "\n",
    "# downsample the data of non-depression class to fit the number of the depression class\n",
    "short_text_0_down = short_text[short_text['Label'] == 0].sample(len(short_text_1), replace=False)\n",
    "\n",
    "# concatenate back the data into 1 dataset\n",
    "frames = [short_text_0_down, short_text_1]\n",
    "short_text_balanced = pd.concat(frames)\n",
    "\n",
    "# shuffle dataset for better data distribution\n",
    "short_text_balanced = short_text_balanced.sample(frac=1).reset_index(drop=True)\n",
    "short_text_balanced['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a1fc32e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    667\n",
      "0    667\n",
      "Name: Label, dtype: int64\n",
      "1    167\n",
      "0    167\n",
      "Name: Label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# train dev split the data with stratified sampling\n",
    "short_text_train, short_text_dev = train_test_split(short_text_balanced, \n",
    "                                                train_size=0.8, \n",
    "                                                random_state=42, \n",
    "                                                stratify=short_text_balanced['Label'])\n",
    "print(short_text_train['Label'].value_counts())\n",
    "print(short_text_dev['Label'].value_counts())\n",
    "\n",
    "# export both the train and the dev data\n",
    "short_text_train.to_csv('./data/emotion_classification/emotion_classification_cleaned_short_text_train.csv', index=False)\n",
    "short_text_dev.to_csv('./data/emotion_classification/emotion_classification_cleaned_short_text_dev.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cda28a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "22a0223f",
   "metadata": {},
   "source": [
    "### Long Dataset for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9e838547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplication of entries\n",
    "long_text=long_text.drop_duplicates(subset={\"Text\",\"Label\"}, \n",
    "                                              keep='first', inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2b1b7691",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 2722/2722 [00:06<00:00, 407.08it/s]\n"
     ]
    }
   ],
   "source": [
    "# cleaning of data\n",
    "text_list = []\n",
    "for i in tqdm(long_text['Text']):\n",
    "    temp = deep_clean(i)\n",
    "    text_list.append(temp)\n",
    "long_text['text_cleaned'] = text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2f4a5773",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    1436\n",
       "0    1286\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove sentences with 1 word or less after stopword removal\n",
    "long_text = long_text[long_text['text_cleaned'].str.split().str.len().gt(1)]\n",
    "\n",
    "# save the temporary backup file\n",
    "long_text.to_csv('./data/backup_cleaned_data/long_text.csv', index=False)\n",
    "\n",
    "# check for data imbalance\n",
    "#print(toy_data.head(2))\n",
    "long_text['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4e8712e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    1286\n",
      "0    1286\n",
      "Name: Label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# long text data\n",
    "long_text_0 = long_text[long_text['Label'] == 0]\n",
    "long_text_1 = long_text[long_text['Label'] == 1]\n",
    "\n",
    "# downsample the data of non-depression class to fit the number of the depression class\n",
    "long_text_1_down = long_text[long_text['Label'] == 1].sample(len(long_text_0), replace=False)\n",
    "\n",
    "# concatenate back the data into 1 dataset\n",
    "frames = [long_text_0, long_text_1_down]\n",
    "long_text_balanced = pd.concat(frames)\n",
    "\n",
    "# shuffle dataset for better data distribution\n",
    "long_text_balanced = long_text_balanced.sample(frac=1).reset_index(drop=True)\n",
    "print(long_text_balanced['Label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5a302fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    1029\n",
      "0    1028\n",
      "Name: Label, dtype: int64\n",
      "0    258\n",
      "1    257\n",
      "Name: Label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# train dev split the data with stratified sampling\n",
    "long_text_train, long_text_dev = train_test_split(long_text_balanced, \n",
    "                                                train_size=0.8, \n",
    "                                                random_state=42, \n",
    "                                                stratify=long_text_balanced['Label'])\n",
    "print(long_text_train['Label'].value_counts())\n",
    "print(long_text_dev['Label'].value_counts())\n",
    "\n",
    "# export both the train and the dev data\n",
    "long_text_train.to_csv('./data/emotion_classification/emotion_classification_cleaned_long_text_train.csv', index=False)\n",
    "long_text_dev.to_csv('./data/emotion_classification/emotion_classification_cleaned_long_text_dev.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc064427",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5e0d954e",
   "metadata": {},
   "source": [
    "### Combination of Long and Short Dataset for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "de40b334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the train set from both datasets defined above\n",
    "combined_text_train = pd.concat([short_text_train, long_text_train])\n",
    "combined_text_dev = pd.concat([short_text_dev, long_text_dev])\n",
    "\n",
    "# shuffle the concatenated datasets for consistency\n",
    "combined_text_train = combined_text_train.sample(frac=1).reset_index(drop=True)\n",
    "combined_text_dev = combined_text_dev.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d4a96795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    1696\n",
      "0    1695\n",
      "Name: Label, dtype: int64\n",
      "0    425\n",
      "1    424\n",
      "Name: Label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# check the stats of the concatenated datasets\n",
    "print(combined_text_train['Label'].value_counts())\n",
    "print(combined_text_dev['Label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c961bfc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export both the train and the dev data\n",
    "combined_text_train.to_csv('./data/emotion_classification/emotion_classification_cleaned_combined_text_train.csv', index=False)\n",
    "combined_text_dev.to_csv('./data/emotion_classification/emotion_classification_cleaned_combined_text_dev.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6168a569",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ba1c1b56",
   "metadata": {},
   "source": [
    "### Short Dataset for Emotion Intensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "02518fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplication of entries\n",
    "short_text_only_depressed=short_text_only_depressed.drop_duplicates(subset={\"Text\",\"Label\"}, \n",
    "                                                                    keep='first', inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8c3734a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 843/843 [00:00<00:00, 1816.84it/s]\n"
     ]
    }
   ],
   "source": [
    "# cleaning of data\n",
    "text_list = []\n",
    "for i in tqdm(short_text_only_depressed['Text']):\n",
    "    temp = deep_clean(i)\n",
    "    text_list.append(temp)\n",
    "short_text_only_depressed['text_cleaned'] = text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "904bf6cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    834\n",
      "Name: Label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# remove sentences with 1 word or less after stopword removal\n",
    "short_text_only_depressed = short_text_only_depressed[short_text_only_depressed['text_cleaned'].str.split().str.len().gt(1)]\n",
    "\n",
    "# check the data entry\n",
    "print(short_text_only_depressed['Label'].value_counts())\n",
    "\n",
    "# save the final file\n",
    "short_text_only_depressed.to_csv('./data/emotion_intensity/emotion_intensity_depressed_clean_short_data_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda95357",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3b16b857",
   "metadata": {},
   "source": [
    "### Long Dataset for Emotion Intensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "190b85d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplication of entries\n",
    "long_text_only_depressed=long_text_only_depressed.drop_duplicates(subset={\"Text\",\"Label\"}, \n",
    "                                                                    keep='first', inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2ae9eb8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1436/1436 [00:03<00:00, 435.15it/s]\n"
     ]
    }
   ],
   "source": [
    "# cleaning of data\n",
    "text_list = []\n",
    "for i in tqdm(long_text_only_depressed['Text']):\n",
    "    temp = deep_clean(i)\n",
    "    text_list.append(temp)\n",
    "long_text_only_depressed['text_cleaned'] = text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "13d979df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    1436\n",
      "Name: Label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# remove sentences with 1 word or less after stopword removal\n",
    "long_text_only_depressed = long_text_only_depressed[long_text_only_depressed['text_cleaned'].str.split().str.len().gt(1)]\n",
    "\n",
    "# check the data entry\n",
    "print(long_text_only_depressed['Label'].value_counts())\n",
    "\n",
    "# save the final file\n",
    "long_text_only_depressed.to_csv('./data/emotion_intensity/emotion_intensity_depressed_clean_long_data_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60dccb38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "adb8cd2b",
   "metadata": {},
   "source": [
    "### WASSA anger dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d7614e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplication of entries\n",
    "wassa_anger_train_raw=wassa_anger_train_raw.drop_duplicates(subset={\"Text\",\"Label\",\"Score\"}, \n",
    "                                                                    keep='first', inplace=False)\n",
    "wassa_anger_dev_raw=wassa_anger_dev_raw.drop_duplicates(subset={\"Text\",\"Label\",\"Score\"}, \n",
    "                                                                    keep='first', inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e01c21d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 857/857 [00:00<00:00, 1961.09it/s]\n"
     ]
    }
   ],
   "source": [
    "# cleaning of data train\n",
    "text_list = []\n",
    "for i in tqdm(wassa_anger_train_raw['Text']):\n",
    "    temp = deep_clean(i)\n",
    "    text_list.append(temp)\n",
    "wassa_anger_train_raw['text_cleaned'] = text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c31d51b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 84/84 [00:00<00:00, 1423.72it/s]\n"
     ]
    }
   ],
   "source": [
    "# cleaning of data dev\n",
    "text_list = []\n",
    "for i in tqdm(wassa_anger_dev_raw['Text']):\n",
    "    temp = deep_clean(i)\n",
    "    text_list.append(temp)\n",
    "wassa_anger_dev_raw['text_cleaned'] = text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "97f45edc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anger    83\n",
      "Name: Label, dtype: int64\n",
      "anger    848\n",
      "Name: Label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# remove sentences with 1 word or less after stopword removal\n",
    "wassa_anger_dev_raw = wassa_anger_dev_raw[wassa_anger_dev_raw['text_cleaned'].str.split().str.len().gt(1)]\n",
    "wassa_anger_train_raw = wassa_anger_train_raw[wassa_anger_train_raw['text_cleaned'].str.split().str.len().gt(1)]\n",
    "\n",
    "# check the data entry\n",
    "print(wassa_anger_dev_raw['Label'].value_counts())\n",
    "print(wassa_anger_train_raw['Label'].value_counts())\n",
    "\n",
    "# save the final file\n",
    "wassa_anger_dev_raw.to_csv('./data/emotion_intensity/emotion_intensity_wassa_anger_combined_dev.csv', index=False)\n",
    "wassa_anger_train_raw.to_csv('./data/emotion_intensity/emotion_intensity_wassa_anger_combined_train.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70319d4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "811e8533",
   "metadata": {},
   "source": [
    "### WASSA fear dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f7f38a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplication of entries\n",
    "wassa_fear_train_raw=wassa_fear_train_raw.drop_duplicates(subset={\"Text\",\"Label\",\"Score\"}, \n",
    "                                                                    keep='first', inplace=False)\n",
    "wassa_fear_dev_raw=wassa_fear_dev_raw.drop_duplicates(subset={\"Text\",\"Label\",\"Score\"}, \n",
    "                                                                    keep='first', inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "80835ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 2142/2142 [00:01<00:00, 1883.91it/s]\n"
     ]
    }
   ],
   "source": [
    "# cleaning of data train\n",
    "text_list = []\n",
    "for i in tqdm(wassa_fear_train_raw['Text']):\n",
    "    temp = deep_clean(i)\n",
    "    text_list.append(temp)\n",
    "wassa_fear_train_raw['text_cleaned'] = text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9af15e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 110/110 [00:00<00:00, 1392.31it/s]\n"
     ]
    }
   ],
   "source": [
    "# cleaning of data dev\n",
    "text_list = []\n",
    "for i in tqdm(wassa_fear_dev_raw['Text']):\n",
    "    temp = deep_clean(i)\n",
    "    text_list.append(temp)\n",
    "wassa_fear_dev_raw['text_cleaned'] = text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3a2d4fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fear    109\n",
      "Name: Label, dtype: int64\n",
      "fear    2115\n",
      "Name: Label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# remove sentences with 1 word or less after stopword removal\n",
    "wassa_fear_dev_raw = wassa_fear_dev_raw[wassa_fear_dev_raw['text_cleaned'].str.split().str.len().gt(1)]\n",
    "wassa_fear_train_raw = wassa_fear_train_raw[wassa_fear_train_raw['text_cleaned'].str.split().str.len().gt(1)]\n",
    "\n",
    "# check the data entry\n",
    "print(wassa_fear_dev_raw['Label'].value_counts())\n",
    "print(wassa_fear_train_raw['Label'].value_counts())\n",
    "\n",
    "# save the final file\n",
    "wassa_fear_dev_raw.to_csv('./data/emotion_intensity/emotion_intensity_wassa_fear_combined_dev.csv', index=False)\n",
    "wassa_fear_train_raw.to_csv('./data/emotion_intensity/emotion_intensity_wassa_fear_combined_train.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012a5c31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2f9f2aec",
   "metadata": {},
   "source": [
    "### WASSA sadness dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cab6c417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplication of entries\n",
    "wassa_sadness_train_raw=wassa_sadness_train_raw.drop_duplicates(subset={\"Text\",\"Label\",\"Score\"}, \n",
    "                                                                    keep='first', inplace=False)\n",
    "wassa_sadness_dev_raw=wassa_sadness_dev_raw.drop_duplicates(subset={\"Text\",\"Label\",\"Score\"}, \n",
    "                                                                    keep='first', inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8ee665ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 1459/1459 [00:00<00:00, 1927.29it/s]\n"
     ]
    }
   ],
   "source": [
    "# cleaning of data train\n",
    "text_list = []\n",
    "for i in tqdm(wassa_sadness_train_raw['Text']):\n",
    "    temp = deep_clean(i)\n",
    "    text_list.append(temp)\n",
    "wassa_sadness_train_raw['text_cleaned'] = text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fe9987dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 74/74 [00:00<00:00, 1541.54it/s]\n"
     ]
    }
   ],
   "source": [
    "# cleaning of data dev\n",
    "text_list = []\n",
    "for i in tqdm(wassa_sadness_dev_raw['Text']):\n",
    "    temp = deep_clean(i)\n",
    "    text_list.append(temp)\n",
    "wassa_sadness_dev_raw['text_cleaned'] = text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3eb0f0cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sadness    74\n",
      "Name: Label, dtype: int64\n",
      "sadness    1444\n",
      "Name: Label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# remove sentences with 1 word or less after stopword removal\n",
    "wassa_sadness_dev_raw = wassa_sadness_dev_raw[wassa_sadness_dev_raw['text_cleaned'].str.split().str.len().gt(1)]\n",
    "wassa_sadness_train_raw = wassa_sadness_train_raw[wassa_sadness_train_raw['text_cleaned'].str.split().str.len().gt(1)]\n",
    "\n",
    "# check the data entry\n",
    "print(wassa_sadness_dev_raw['Label'].value_counts())\n",
    "print(wassa_sadness_train_raw['Label'].value_counts())\n",
    "\n",
    "# save the final file\n",
    "wassa_sadness_dev_raw.to_csv('./data/emotion_intensity/emotion_intensity_wassa_sadness_combined_dev.csv', index=False)\n",
    "wassa_sadness_train_raw.to_csv('./data/emotion_intensity/emotion_intensity_wassa_sadness_combined_train.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86577985",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c9db042f",
   "metadata": {},
   "source": [
    "### text2emotion and VADER dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c8199340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read previously exported data for data consistency\n",
    "long_text_only_depressed_t2e_vader = pd.read_csv('./data/emotion_intensity/emotion_intensity_depressed_clean_long_data_test.csv')\n",
    "short_text_only_depressed_t2e_vader = pd.read_csv('./data/emotion_intensity/emotion_intensity_depressed_clean_short_data_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4a16b317",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1436/1436 [00:03<00:00, 468.63it/s]\n"
     ]
    }
   ],
   "source": [
    "# mild cleanup of the data for the long text\n",
    "text_list = []\n",
    "for i in tqdm(long_text_only_depressed_t2e_vader['Text']):\n",
    "    temp = vader_and_t2e_clean(i)\n",
    "    text_list.append(temp)\n",
    "long_text_only_depressed_t2e_vader['text_cleaned_t2e_vader'] = text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "25705d9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 834/834 [00:00<00:00, 1930.42it/s]\n"
     ]
    }
   ],
   "source": [
    "# mild cleanup of the data for the short text\n",
    "text_list = []\n",
    "for i in tqdm(short_text_only_depressed_t2e_vader['Text']):\n",
    "    temp = vader_and_t2e_clean(i)\n",
    "    text_list.append(temp)\n",
    "short_text_only_depressed_t2e_vader['text_cleaned_t2e_vader'] = text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "58588f51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    1436\n",
      "Name: Label, dtype: int64\n",
      "1    834\n",
      "Name: Label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# check the data entry\n",
    "print(long_text_only_depressed_t2e_vader['Label'].value_counts())\n",
    "print(short_text_only_depressed_t2e_vader['Label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ea46f8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the final file\n",
    "long_text_only_depressed_t2e_vader.to_csv('./data/emotion_intensity/emotion_intensity_depressed_clean_long_data_vader_t2e.csv', index=False)\n",
    "short_text_only_depressed_t2e_vader.to_csv('./data/emotion_intensity/emotion_intensity_depressed_clean_short_data_vader_t2e.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd92cff4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3d228127",
   "metadata": {},
   "source": [
    "### Data for ECPE dataset (long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f4e64397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read long text only for ECPE dataset\n",
    "long_text_only_depressed_ecpe = pd.read_csv('./data/long_text_only_depressed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "16c29fef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1437/1437 [00:01<00:00, 973.22it/s]\n"
     ]
    }
   ],
   "source": [
    "# cleanup of the data for the ecpe task\n",
    "text_list = []\n",
    "for i in tqdm(long_text_only_depressed_ecpe['Text']):\n",
    "    temp = ecpe_clean(i)\n",
    "    text_list.append(temp)\n",
    "long_text_only_depressed_ecpe['text_cleaned_ecpe'] = text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8987f9ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    1437\n",
      "Name: Label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# check the data entry\n",
    "print(long_text_only_depressed_ecpe['Label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "dc0de285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the final file\n",
    "long_text_only_depressed_ecpe.to_csv('./data/ecpe/ecpe_cleaned_long_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe68a04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "05c4a8f8",
   "metadata": {},
   "source": [
    "### Data for ECPE dataset (short)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108d4104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read long text only for ECPE dataset\n",
    "short_text_only_depressed_ecpe = pd.read_csv('./data/short_text_only_depressed.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
