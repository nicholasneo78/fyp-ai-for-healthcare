{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c6da78a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\nicholasneo78\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'word'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import preprocessor as p # tweet-preprocessor\n",
    "import cleantext\n",
    "from autocorrect import Speller\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.stem.WordNetLemmatizer().lemmatize('word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e71f2139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the helper functions\n",
    "\n",
    "# https://en.wikipedia.org/wiki/Wikipedia%3aList_of_English_contractions\n",
    "# remove contractions\n",
    "def contraction_removal(phrase):\n",
    "    # replace bad characters\n",
    "    phrase = phrase.replace(u'’', u\"'\")\n",
    "    phrase = phrase.replace(u'‘', u\"'\")\n",
    "    # more specific change\n",
    "    phrase = re.sub(r\"won\\'t\", \" will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \" cannot\", phrase)\n",
    "    phrase = re.sub(r\"shan\\'t\", \" shall not\", phrase)\n",
    "    phrase = re.sub(r\"I ain\\t\", \" I am not\", phrase)\n",
    "    phrase = re.sub(r\"i ain\\t\", \" I am not\", phrase)\n",
    "    phrase = re.sub(r\"She ain\\t\", \" she is not\", phrase)\n",
    "    phrase = re.sub(r\"He ain\\t\", \" he is not\", phrase)\n",
    "    phrase = re.sub(r\"he ain\\t\", \" he am not\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase\n",
    "\n",
    "# remove url, mention, reserved words, emoji, smiley and number\n",
    "# using tweet preprocessor library here\n",
    "def tweet_preprocessor(text, config):\n",
    "    if config == 'deep_clean' or config == 'ecpe':\n",
    "        p.set_options(p.OPT.URL, p.OPT.MENTION, p.OPT.EMOJI, p.OPT.RESERVED, p.OPT.SMILEY)\n",
    "    elif config == 'vader':\n",
    "        p.set_options(p.OPT.URL, p.OPT.MENTION, p.OPT.RESERVED, p.OPT.SMILEY)\n",
    "    text = p.clean(text)\n",
    "    # remove the url starting with www\n",
    "    text = re.sub(r\"\\bwww.\\w+\", \"\", text)\n",
    "    # just remove hashtag (not the whole hashtag and words)\n",
    "    text = re.sub(r\"#\", \" \", text)\n",
    "    return text\n",
    "\n",
    "# remove extra spaces and stopwords, do lowercase, and remove punctuations.\n",
    "def clean_text(text, removeLower=True, removeNumbers=True, removePunct=True, removeExtraSpace=True):\n",
    "    text = cleantext.clean(text, \n",
    "                    lowercase=removeLower, \n",
    "                    numbers=removeNumbers, \n",
    "                    punct=removePunct,\n",
    "                    extra_spaces=removeExtraSpace)\n",
    "    return text\n",
    "\n",
    "# keep only alphabets (only for deep clean)\n",
    "def keep_alphabet_only(text):\n",
    "    return re.sub('[^a-zA-Z- ]+', '', text)\n",
    "\n",
    "# keep alphabets, some basic punctuations and numbera\n",
    "def keep_selected(text):\n",
    "    emoji_pat = '[\\U0001F300-\\U0001F64F\\U0001F680-\\U0001F6FF\\u2600-\\u26FF\\u2700-\\u27BF]'\n",
    "    shrink_whitespace_reg = re.compile(r'\\s{2,}')\n",
    "    reg = re.compile(r'({})|[^a-zA-Z0-9,.!?-]'.format(emoji_pat)) # line a\n",
    "    result = reg.sub(lambda x: ' {} '.format(x.group(1)) if x.group(1) else ' ', text)\n",
    "    return shrink_whitespace_reg.sub(' ', result)\n",
    "\n",
    "# eliminate letters who appeared more than twice in the text\n",
    "def eliminate_multi_letters(text):\n",
    "    return re.sub(r'(.)\\1{2,}', r'\\1\\1', text)\n",
    "\n",
    "# autocorrect to fix spelling errors\n",
    "spell = Speller()\n",
    "def autocorrect(text):\n",
    "    return spell(text)\n",
    "\n",
    "# perform lemmatization here\n",
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "# lemmatize text\n",
    "def lemmatize_text(text):\n",
    "    text_list = [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)]\n",
    "    listToStr = ' '.join([str(elem) for elem in text_list])\n",
    "    return listToStr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982d05b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6b9e02b1",
   "metadata": {},
   "source": [
    "## Deep Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26db301c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spell = Speller()\n",
    "\n",
    "# perform lemmatization here\n",
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61665beb",
   "metadata": {},
   "source": [
    "To generate the following files:   \n",
    "emotion_classification_cleaned_long_data_{type}.csv (train and dev)   \n",
    "emotion_classification_cleaned_short_data_{type}.csv (train and dev)  \n",
    "emotion_classification_cleaned_toy_data_{type}.csv (train and dev)  \n",
    "emotion_classification_cleaned_combined_data_{type}.csv (train and dev)      \n",
    "  \n",
    "emotion_intensity_wassa_sadness_combined_{type}.csv (train and dev)   \n",
    "emotion_intensity_wassa_anger_combined_{type}.csv (train and dev)   \n",
    "emotion_intensity_wassa_fear_combined_{type}.csv (train and dev)   \n",
    "   \n",
    "Only depressed data here\n",
    "emotion_intensity_depressed_clean_long_data_test.csv (test set for machine learning portion)  \n",
    "emotion_intensity_depressed_clean_short_data_test.csv (test set for machine learning portion)  \n",
    "  \n",
    "Total: 16 data  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8a6a278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the deep clean data\n",
    "def deep_clean(text):\n",
    "    text = contraction_removal(text)\n",
    "    text = tweet_preprocessor(text, 'deep_clean')\n",
    "    text = clean_text(text, \n",
    "                      removeLower=True, \n",
    "                      removeNumbers=True, \n",
    "                      removePunct=True, \n",
    "                      removeExtraSpace=True)\n",
    "    text = keep_alphabet_only(text)\n",
    "    text = eliminate_multi_letters(text)\n",
    "    text = autocorrect(text)\n",
    "    text = lemmatize_text(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8804e046",
   "metadata": {},
   "source": [
    "## Vader and text2emotion data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0215fd3",
   "metadata": {},
   "source": [
    "To generate the following files:  \n",
    "emotion_intensity_depressed_clean_long_data_vader_t2e.csv\n",
    "emotion_intensity_depressed_clean_short_data_vader_t2e.csv\n",
    "\n",
    "Total: 2 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "248b18f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the vader and t2e data\n",
    "def vader_and_t2e_clean(text):\n",
    "    text = contraction_removal(text)\n",
    "    text = tweet_preprocessor(text, 'vader')\n",
    "    text = clean_text(text, \n",
    "                      removeLower=False, \n",
    "                      removeNumbers=True, \n",
    "                      removePunct=False, \n",
    "                      removeExtraSpace=True)\n",
    "    text = keep_selected(text)\n",
    "    text = eliminate_multi_letters(text)\n",
    "    text = autocorrect(text)\n",
    "    text = lemmatize_text(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7e5714",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5959c36e",
   "metadata": {},
   "source": [
    "## ECPE data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08be4310",
   "metadata": {},
   "source": [
    "To generate the following file:  \n",
    "ecpe_cleaned_long_data.csv\n",
    "\n",
    "Total: 1 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71a044d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ecpe_clean(text):\n",
    "    text = contraction_removal(text)\n",
    "    text = tweet_preprocessor(text, 'ecpe')\n",
    "    text = clean_text(text, \n",
    "                      removeLower=False, \n",
    "                      removeNumbers=False, \n",
    "                      removePunct=False, \n",
    "                      removeExtraSpace=True)\n",
    "    text = eliminate_multi_letters(text)\n",
    "    text = autocorrect(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69d71e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "332b5129",
   "metadata": {},
   "source": [
    "## Import the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a815eb5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da24bec0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0642126c",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ac8621",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1581ce5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c16e133f",
   "metadata": {},
   "source": [
    "## Save the Data Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880814a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60d436a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054b569b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
